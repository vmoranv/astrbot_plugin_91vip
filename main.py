import os
import json
import uuid
import time
import argparse
import csv
import re
import random
import asyncio
import shutil
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from itertools import count
from threading import Lock
from urllib.parse import urlparse

from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register, StarTools
from astrbot.api import logger
from astrbot.api import AstrBotConfig

# çˆ¬è™«ç›¸å…³å¯¼å…¥
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

# å¼‚æ­¥HTTPå’Œå›¾ç‰‡å¤„ç†å¯¼å…¥
import aiohttp
from PIL import Image

# çˆ¬è™«ç›¸å…³å¸¸é‡å’Œç±»
VIDEO_CARD_SELECTOR = ".well-sm.videos-text-align"
TITLE_SELECTOR = ".video-title"

@dataclass
class VideoRecord:
    link: str
    title: str

@dataclass
class TaskStatus:
    """ä»»åŠ¡çŠ¶æ€"""
    task_id: str
    task_type: str
    status: str  # "running", "completed", "failed", "cancelled"
    progress: int  # 0-100
    message: str
    result: Optional[Any] = None
    error: Optional[str] = None


# 91pornçˆ¬è™«å‡½æ•°
def build_page_url(category: str, viewtype: str, page: int) -> str:
    return f"https://91porn.com/v.php?category={category}&viewtype={viewtype}&page={page}"

def fetch_page(
    session: requests.Session,
    category: str,
    viewtype: str,
    page: int,
    timeout: float,
    delay: float,
) -> List[VideoRecord]:
    url = build_page_url(category, viewtype, page)
    time.sleep(delay)
    try:
        # å¢å¼ºè¯·æ±‚å¤´ä»¥é¿å…403é”™è¯¯
        enhanced_headers = {
            'User-Agent': session.headers.get('User-Agent',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://91porn.com/',
            'DNT': '1',
            'Sec-GPC': '1'
        }
        
        response = session.get(url, timeout=timeout, headers=enhanced_headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        cards = soup.select(VIDEO_CARD_SELECTOR)
        results: List[VideoRecord] = []
        seen_links = set()
        for card in cards:
            link_el = card.find("a")
            title_el = card.select_one(TITLE_SELECTOR)
            link = getattr(link_el, "get", lambda *_: None)("href")
            title = title_el.text.strip() if title_el and title_el.text else "æ— æ ‡é¢˜"
            if not link or not title or link in seen_links:
                continue
            seen_links.add(link)
            results.append(VideoRecord(link=link, title=title))
        return results
    except requests.RequestException as e:
        logger.debug("Request failed for page %d: %s", page, e)
        raise

def crawl_91porn(args: argparse.Namespace) -> None:
    """å¢å¼ºç‰ˆ91pornçˆ¬è™«å‡½æ•°ï¼Œæ”¯æŒæ›´å¥½çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•"""
    session = requests.Session()
    session.headers.update({"User-Agent": args.user_agent})

    logger.info(
        "Starting crawl: category=%s, viewtype=%s, start_page=%s, max_pages=%s, workers=%s",
        args.category,
        args.viewtype,
        args.start_page,
        args.max_pages,
        args.workers,
    )

    csv_writer = None
    jsonl_file = None
    csv_file = None
    lock = Lock()
    total_records = 0
    pages_processed = 0
    empty_pages_in_a_row = 0
    failed_pages = 0

    try:
        # éªŒè¯è¾“å‡ºæ–‡ä»¶è·¯å¾„
        if args.output_csv:
            try:
                csv_file = open(args.output_csv, "w", encoding="utf-8", newline="")
                fieldnames = list(VideoRecord.__annotations__.keys())
                csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
                csv_writer.writeheader()
                logger.info(f"CSVè¾“å‡ºæ–‡ä»¶å·²åˆ›å»º: {args.output_csv}")
            except Exception as e:
                logger.error(f"åˆ›å»ºCSVæ–‡ä»¶å¤±è´¥: {e}")
                raise

        if args.output_jsonl:
            try:
                jsonl_file = open(args.output_jsonl, "w", encoding="utf-8")
                logger.info(f"JSONLè¾“å‡ºæ–‡ä»¶å·²åˆ›å»º: {args.output_jsonl}")
            except Exception as e:
                logger.error(f"åˆ›å»ºJSONLæ–‡ä»¶å¤±è´¥: {e}")
                raise

        with ThreadPoolExecutor(max_workers=args.workers) as executor, tqdm(
            desc="Crawling pages", unit="page", total=args.max_pages
        ) as pbar:
            page_generator = count(args.start_page)
            futures = {
                executor.submit(
                    fetch_page,
                    session,
                    args.category,
                    args.viewtype,
                    next(page_generator),
                    args.timeout,
                    args.delay,
                )
                for _ in range(args.workers)
            }

            while futures:
                done, futures = as_completed(futures), set()

                for future in done:
                    try:
                        records = future.result()
                        pages_processed += 1
                        pbar.update(1)

                        if records:
                            empty_pages_in_a_row = 0
                            logger.debug(f"é¡µé¢ {pages_processed} è·å–åˆ° {len(records)} æ¡è®°å½•")
                            
                            with lock:
                                if csv_writer and csv_file:
                                    for record in records:
                                        csv_writer.writerow(asdict(record))
                                if jsonl_file:
                                    for record in records:
                                        jsonl_file.write(
                                            json.dumps(asdict(record), ensure_ascii=False)
                                            + "\n"
                                        )
                                total_records += len(records)
                        else:
                            empty_pages_in_a_row += 1
                            logger.warning(f"é¡µé¢ {pages_processed} æ²¡æœ‰è·å–åˆ°æ•°æ®")

                        pbar.set_postfix(records=f"{total_records}", failed=f"{failed_pages}")

                    except Exception as e:
                        logger.error(f"é¡µé¢ {pages_processed} è·å–å¤±è´¥: {e}")
                        failed_pages += 1
                        empty_pages_in_a_row += 1

                    # æ£€æŸ¥åœæ­¢æ¡ä»¶
                    stop_condition_met = False
                    if args.max_pages and pages_processed >= args.max_pages:
                        logger.info(f"è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {args.max_pages}")
                        stop_condition_met = True
                    
                    if empty_pages_in_a_row >= args.stop_on_empty_pages:
                        logger.info(f"è¿ç»­ {args.stop_on_empty_pages} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                        stop_condition_met = True

                    if stop_condition_met:
                        logger.info("åœæ­¢æ¡ä»¶æ»¡è¶³ï¼Œä¸å†æäº¤æ–°ä»»åŠ¡")
                        continue

                    # æäº¤æ–°ä»»åŠ¡
                    try:
                        futures.add(
                            executor.submit(
                                fetch_page,
                                session,
                                args.category,
                                args.viewtype,
                                next(page_generator),
                                args.timeout,
                                args.delay,
                            )
                        )
                    except Exception as e:
                        logger.error(f"æäº¤æ–°ä»»åŠ¡å¤±è´¥: {e}")

                # æ¸…ç†å‰©ä½™ä»»åŠ¡
                if not futures:
                    logger.debug("æ¸…ç†å‰©ä½™ä»»åŠ¡...")
                    for f in executor._threads:
                        try:
                            if hasattr(f, '_work_queue') and f._work_queue:
                                f._work_queue.queue.clear()
                        except Exception as e:
                            logger.debug(f"æ¸…ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")

    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­çˆ¬å–è¿‡ç¨‹")
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        raise
    finally:
        # æ¸…ç†èµ„æº
        try:
            if csv_file:
                csv_file.close()
                logger.info("CSVæ–‡ä»¶å·²å…³é—­")
            if jsonl_file:
                jsonl_file.close()
                logger.info("JSONLæ–‡ä»¶å·²å…³é—­")
            session.close()
            logger.info("HTTPä¼šè¯å·²å…³é—­")
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")

    logger.info(
        "çˆ¬å–å®Œæˆã€‚å¤„ç†é¡µé¢: %d, æˆåŠŸè®°å½•: %d, å¤±è´¥é¡µé¢: %d",
        pages_processed,
        total_records,
        failed_pages,
    )

# è§†é¢‘ä¸‹è½½ç›¸å…³å‡½æ•°
def get_one_page_urls(r):
    one_page_video_urls = []
    soup = BeautifulSoup(r.text, 'html.parser')
    elements = soup.select(".has-text-grey-dark")
    for e in elements[0::2]:
        one_page_video_urls.append(e["href"])
    return one_page_video_urls

def get_video_ids(r):
    ids = []
    soup = BeautifulSoup(r.text, 'html.parser')
    for i in soup.find_all(name='img', attrs={'loading': 'lazy'}):
        ids.append(re.search(r'/(\d+)\.webp$', i.get('src')).group()[1:-5])
    return ids

def get_video_info(r):
    soup = BeautifulSoup(r.text, 'html.parser')
    m3u8_pattern = r'm3u8\?t=([^&]+)&m=([A-Za-z0-9_\-]+)'
    favorites_pattern = r'"favorites":\d+,'
    m3u8 = re.search(m3u8_pattern, r.text).group()
    favorites = re.search(favorites_pattern, r.text).group()
    title = soup.find(name='meta', attrs={'property': 'twitter:title'}).get('content')
    uploader = soup.find(name='meta', attrs={'property': 'twitter:creator'}).get('content')
    date_pattern = "(([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|" + "((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|" + "((0[48]|[2468][048]|[3579][26])00))-02-29)$"
    upload_date = re.search(date_pattern, soup.select(".content.is-size-7")[0].text).group()
    return m3u8, title, favorites, uploader, upload_date

def del_trash(r, one_page_video_urls, ids):
    del_urls = []
    del_ids = []
    soup = BeautifulSoup(r.text, 'html.parser')
    video_duration = soup.select(".duration")
    for i in range(len(video_duration)):
        if int(video_duration[i].text[3:5]) >= 20:
            del_urls.append(one_page_video_urls[i])
            del_ids.append(ids[i])
    pure_urls = list(filter(lambda x: x not in del_urls, one_page_video_urls))
    pure_ids = list(filter(lambda x: x not in del_ids, ids))
    return pure_urls, pure_ids

def download_videos_func(pages: str, max_duration: int, downloads_dir: str):
    """è§†é¢‘ä¸‹è½½å‡½æ•°"""
    base_url = 'https://zvm.xinhua107.com/'
    favorite_url = base_url+'video/category/most-favorite/'
    # cdns = ["cdn2.jiuse3.cloud","fdc100g2b.jiuse.cloud","dp.jiuse.cloud","shark10g2.jiuse.cloud"]  # æš‚æ—¶æ³¨é‡Šæ‰æœªä½¿ç”¨çš„å˜é‡
    
    # è§£æé¡µé¢èŒƒå›´
    if "-" in pages:
        start_page, end_page = map(int, pages.split("-"))
        page_range = range(start_page, end_page + 1)
    else:
        page_range = range(int(pages), int(pages) + 1)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    downloaded_files = []
    
    for page in page_range:
        r_page = requests.get(favorite_url + str(page), headers=headers)

        #è·å–å½“å‰é¡µé¢æ‰€æœ‰è§†é¢‘çš„é“¾æ¥ï¼Œç”¨æ¥è¿›å…¥æ¯ä¸ªè§†é¢‘çš„é¡µé¢
        #è¿”å›çš„urlæ²¡æœ‰baseï¼Œè¿™é‡Œå¤„ç†ä¸€ä¸‹
        t_one_page_video_urls = get_one_page_urls(r_page)
        t2_one_page_video_urls = [base_url+t for t in t_one_page_video_urls]
        #è·å–å½“å‰é¡µé¢æ‰€æœ‰è§†é¢‘çš„idï¼Œç”¨æ¥ä¸‹è½½m3u8å’Œtsæ–‡ä»¶ï¼Œéœ€è¦æ‹¼æ¥è¿™ä¸¤ä¸ªçš„é“¾æ¥
        t_ids = get_video_ids(r_page)
        one_page_video_urls, ids = del_trash(r_page, t2_one_page_video_urls, t_ids)

        #æ¥ä¸‹æ¥è¿›å…¥æ¯ä¸ªè§†é¢‘çš„é¡µé¢è¿›è¡Œä¸‹è½½ã€‚è¿™é‡Œéœ€è¦éå†è§†é¢‘ä¸»é¡µå’Œè§†é¢‘idæ‰€ä»¥ç”¨forå¾ªç¯
        for i in range(len(one_page_video_urls)):
            print(f'processing page {page} video {i}')
            r_video = requests.get(one_page_video_urls[i], headers=headers)

            #è·å–è§†é¢‘çš„ä¿¡æ¯
            m3u8, title, favorites, uploader, upload_date = get_video_info(r_video)
            print(title)
            # m3u8_url = 'https://'+cdns[0]+'/hls/' + ids[i] + '/index.'+m3u8
            # r_m3u8 = requests.get(m3u8_url, headers=headers)  # æ³¨é‡Šæ‰æœªä½¿ç”¨çš„è¯·æ±‚

            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªè®°å½•ä¸‹è½½ä¿¡æ¯è€Œä¸å®é™…ä¸‹è½½
            # å®é™…ä¸‹è½½éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼ŒåŒ…æ‹¬tsæ–‡ä»¶ä¸‹è½½å’Œåˆå¹¶
            title_clean = title.replace('/', '').replace('\\', '')
            file_name = f'{page}-{i}-{title_clean}.mp4'
            file_path = os.path.join(downloads_dir, file_name)
            
            # æ¨¡æ‹Ÿä¸‹è½½å®Œæˆ
            downloaded_files.append({
                'title': title,
                'file_path': file_path,
                'page': page,
                'index': i
            })
    
    return downloaded_files


@register("91vip", "91VIP", "91pornè§†é¢‘çˆ¬è™«æ’ä»¶", "1.0.0", "https://github.com/your-repo/astrbot_plugin_91vip")
class MyPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config
        
        # ä½¿ç”¨StarToolsè·å–æ•°æ®ç›®å½• - æŒ‰ç…§å‚è€ƒä»£ç çš„æ–¹å¼
        data_dir = StarTools.get_data_dir("astrbot_plugin_91vip")
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®å­ç›®å½•
        self.data_dir = str(data_dir)
        self.outputs_dir = os.path.join(self.data_dir, "outputs")
        self.downloads_dir = os.path.join(self.data_dir, "downloads")
        self.temp_dir = os.path.join(self.data_dir, "temp")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.outputs_dir, exist_ok=True)
        os.makedirs(self.downloads_dir, exist_ok=True)
        os.makedirs(self.temp_dir, exist_ok=True)
        
        # ä»»åŠ¡ç®¡ç†
        self.tasks: Dict[str, TaskStatus] = {}
        self.executor = ThreadPoolExecutor(max_workers=config.get("max_concurrent_tasks", 2))
        
        # å¼‚æ­¥HTTPå®¢æˆ·ç«¯
        self.http_client: Optional[aiohttp.ClientSession] = None
        
        logger.info("91pornçˆ¬è™«æ’ä»¶åˆå§‹åŒ–å®Œæˆ")

    async def initialize(self):
        """å¯é€‰æ‹©å®ç°å¼‚æ­¥çš„æ’ä»¶åˆå§‹åŒ–æ–¹æ³•ï¼Œå½“å®ä¾‹åŒ–è¯¥æ’ä»¶ç±»ä¹‹åä¼šè‡ªåŠ¨è°ƒç”¨è¯¥æ–¹æ³•ã€‚"""
        await self.initialize_async()
        logger.info("91pornçˆ¬è™«æ’ä»¶å¼‚æ­¥åˆå§‹åŒ–å®Œæˆ")

    async def initialize_async(self):
        """å¼‚æ­¥åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯"""
        try:
            # åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯
            proxy = self.config.get("proxy", "") if self.config else ""
            timeout = self.config.get("timeout", 30) if self.config else 30

            # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
            headers = {
                'User-Agent': self.config.get("91porn_user_agent",
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }

            connector = aiohttp.TCPConnector(limit=10)
            timeout_config = aiohttp.ClientTimeout(total=timeout)

            # æ­£ç¡®çš„ä»£ç†é…ç½®æ–¹å¼
            if proxy:
                self.http_client = aiohttp.ClientSession(
                    connector=connector,
                    timeout=timeout_config,
                    proxy=proxy,
                    headers=headers
                )
            else:
                self.http_client = aiohttp.ClientSession(
                    connector=connector,
                    timeout=timeout_config,
                    headers=headers
                )

            logger.info("HTTPå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"HTTPå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def _generate_task_id(self) -> str:
        """ç”Ÿæˆä»»åŠ¡ID"""
        return str(uuid.uuid4())[:8]

    def _get_task_status(self, task_id: str) -> Optional[TaskStatus]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.tasks.get(task_id)

    def _get_all_tasks(self) -> Dict[str, TaskStatus]:
        """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
        return self.tasks.copy()

    def _cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        if task_id in self.tasks:
            self.tasks[task_id].status = "cancelled"
            return True
        return False

    def _run_91porn_scraper(self, task_id: str, category: str, max_pages: int, output_format: str, return_content: bool = False):
        """è¿è¡Œå¢å¼ºç‰ˆ91pornçˆ¬è™«ï¼Œæ”¯æŒæ›´å¥½çš„è¿›åº¦è·Ÿè¸ªå’Œé”™è¯¯å¤„ç†"""
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.tasks[task_id].progress = 5
            self.tasks[task_id].message = "å‡†å¤‡çˆ¬å–å‚æ•°..."
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
            timestamp = int(time.time())
            
            if output_format == "jsonl":
                output_file = os.path.join(self.outputs_dir, f"91porn_{category}_{timestamp}.jsonl") if not return_content else None
                csv_file = None
            else:
                output_file = os.path.join(self.outputs_dir, f"91porn_{category}_{timestamp}.csv") if not return_content else None
                csv_file = output_file
                output_file = None
            
            # éªŒè¯è¾“å‡ºç›®å½•
            if not return_content:
                try:
                    os.makedirs(self.outputs_dir, exist_ok=True)
                except Exception as e:
                    raise Exception(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {e}")
            
            # æ„å»ºå¢å¼ºç‰ˆå‚æ•°
            import argparse
            args = argparse.Namespace()
            args.category = category
            args.viewtype = self.config.get("91porn_viewtype", "basic")
            args.start_page = self.config.get("91porn_start_page", 1)
            args.max_pages = max_pages
            args.workers = self.config.get("91porn_workers", 3)
            args.delay = self.config.get("91porn_delay", 1.0)
            args.timeout = self.config.get("91porn_timeout", 15.0)
            args.user_agent = self.config.get("91porn_user_agent",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
            args.output_jsonl = output_file
            args.output_csv = csv_file
            args.verbose = self.config.get("91porn_verbose", False)
            args.stop_on_empty_pages = self.config.get("91porn_stop_on_empty_pages", 3)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.tasks[task_id].progress = 15
            self.tasks[task_id].message = f"å¼€å§‹çˆ¬å– {category} åˆ†ç±»æ•°æ®ï¼Œæœ€å¤š {max_pages} é¡µ..."
            
            # è®°å½•çˆ¬å–å¼€å§‹
            start_time = time.time()
            logger.info(f"å¼€å§‹çˆ¬å–ä»»åŠ¡ {task_id}: {category} åˆ†ç±»ï¼Œæœ€å¤š {max_pages} é¡µ")
            
            # å¦‚æœéœ€è¦ç›´æ¥è¿”å›å†…å®¹ï¼Œä½¿ç”¨å†…å­˜çˆ¬å–
            if return_content:
                records = self._crawl_to_memory(args)
                crawl_duration = time.time() - start_time
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                self.tasks[task_id].progress = 100
                self.tasks[task_id].message = f"çˆ¬å–å®Œæˆï¼è€—æ—¶ {crawl_duration:.1f} ç§’ï¼Œè·å– {len(records)} æ¡è®°å½•"
                self.tasks[task_id].result = {
                    "records": records,
                    "records_count": len(records),
                    "crawl_duration": crawl_duration,
                    "category": category,
                    "max_pages": max_pages,
                    "output_format": output_format,
                    "return_content": True
                }
            else:
                # è¿è¡Œå¢å¼ºç‰ˆçˆ¬è™«ï¼ˆä¿å­˜åˆ°æ–‡ä»¶ï¼‰
                crawl_91porn(args)
                
                # è®¡ç®—çˆ¬å–è€—æ—¶
                crawl_duration = time.time() - start_time
                
                # ç»Ÿè®¡ç»“æœæ–‡ä»¶
                records_count = 0
                final_file = output_file or csv_file
                
                if final_file and os.path.exists(final_file):
                    try:
                        if output_format == "jsonl":
                            with open(final_file, 'r', encoding='utf-8') as f:
                                records_count = sum(1 for _ in f)
                        else:  # CSV
                            with open(final_file, 'r', encoding='utf-8') as f:
                                records_count = sum(1 for _ in f) - 1  # å‡å»æ ‡é¢˜è¡Œ
                    except Exception as e:
                        logger.warning(f"ç»Ÿè®¡è®°å½•æ•°é‡å¤±è´¥: {e}")
                        records_count = "æœªçŸ¥"
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                self.tasks[task_id].progress = 100
                self.tasks[task_id].message = f"çˆ¬å–å®Œæˆï¼è€—æ—¶ {crawl_duration:.1f} ç§’ï¼Œè·å– {records_count} æ¡è®°å½•"
                self.tasks[task_id].result = {
                    "output_file": final_file,
                    "records_count": records_count,
                    "crawl_duration": crawl_duration,
                    "category": category,
                    "max_pages": max_pages,
                    "output_format": output_format,
                    "return_content": False
                }
            
            logger.info(f"çˆ¬å–ä»»åŠ¡ {task_id} å®Œæˆ")
            
        except KeyboardInterrupt:
            logger.warning(f"çˆ¬å–ä»»åŠ¡ {task_id} è¢«ç”¨æˆ·ä¸­æ–­")
            self.tasks[task_id].status = "cancelled"
            self.tasks[task_id].message = "çˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­"
        except Exception as e:
            logger.error(f"91pornçˆ¬è™«ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
            self.tasks[task_id].status = "failed"
            self.tasks[task_id].error = str(e)
            self.tasks[task_id].message = f"çˆ¬å–å¤±è´¥: {str(e)}"
        finally:
            if self.tasks[task_id].status not in ["failed", "cancelled"]:
                self.tasks[task_id].status = "completed"

    def _crawl_to_memory(self, args: argparse.Namespace) -> List[VideoRecord]:
        """çˆ¬å–æ•°æ®åˆ°å†…å­˜ï¼Œä¸ä¿å­˜åˆ°æ–‡ä»¶"""
        session = requests.Session()
        session.headers.update({"User-Agent": args.user_agent})
        
        # å¢å¼ºè¯·æ±‚å¤´ä»¥é¿å…403é”™è¯¯
        enhanced_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://91porn.com/',
            'DNT': '1',
            'Sec-GPC': '1'
        }
        session.headers.update(enhanced_headers)
        
        all_records = []
        pages_processed = 0
        empty_pages_in_a_row = 0
        failed_pages = 0
        
        try:
            with ThreadPoolExecutor(max_workers=args.workers) as executor:
                page_generator = count(args.start_page)
                futures = {
                    executor.submit(
                        fetch_page,
                        session,
                        args.category,
                        args.viewtype,
                        next(page_generator),
                        args.timeout,
                        args.delay,
                    )
                    for _ in range(min(args.workers, args.max_pages or args.workers))
                }

                while futures and pages_processed < (args.max_pages or float('inf')):
                    done, futures = as_completed(futures), set()

                    for future in done:
                        try:
                            records = future.result()
                            pages_processed += 1

                            if records:
                                empty_pages_in_a_row = 0
                                all_records.extend(records)
                                logger.debug(f"é¡µé¢ {pages_processed} è·å–åˆ° {len(records)} æ¡è®°å½•")
                            else:
                                empty_pages_in_a_row += 1
                                logger.warning(f"é¡µé¢ {pages_processed} æ²¡æœ‰è·å–åˆ°æ•°æ®")

                            # æ£€æŸ¥åœæ­¢æ¡ä»¶
                            if empty_pages_in_a_row >= args.stop_on_empty_pages:
                                logger.info(f"è¿ç»­ {args.stop_on_empty_pages} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                                break

                            # æäº¤æ–°ä»»åŠ¡
                            if pages_processed < (args.max_pages or float('inf')):
                                try:
                                    futures.add(
                                        executor.submit(
                                            fetch_page,
                                            session,
                                            args.category,
                                            args.viewtype,
                                            next(page_generator),
                                            args.timeout,
                                            args.delay,
                                        )
                                    )
                                except Exception as e:
                                    logger.error(f"æäº¤æ–°ä»»åŠ¡å¤±è´¥: {e}")
                                    break

                        except Exception as e:
                            logger.error(f"é¡µé¢ {pages_processed} è·å–å¤±è´¥: {e}")
                            failed_pages += 1
                            empty_pages_in_a_row += 1

                            # æ£€æŸ¥åœæ­¢æ¡ä»¶
                            if empty_pages_in_a_row >= args.stop_on_empty_pages:
                                logger.info(f"è¿ç»­ {args.stop_on_empty_pages} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                                break

                            # æäº¤æ–°ä»»åŠ¡
                            if pages_processed < (args.max_pages or float('inf')):
                                try:
                                    futures.add(
                                        executor.submit(
                                            fetch_page,
                                            session,
                                            args.category,
                                            args.viewtype,
                                            next(page_generator),
                                            args.timeout,
                                            args.delay,
                                        )
                                    )
                                except Exception as e:
                                    logger.error(f"æäº¤æ–°ä»»åŠ¡å¤±è´¥: {e}")
                                    break

        except Exception as e:
            logger.error(f"å†…å­˜çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        finally:
            session.close()

        logger.info(
            "å†…å­˜çˆ¬å–å®Œæˆã€‚å¤„ç†é¡µé¢: %d, æˆåŠŸè®°å½•: %d, å¤±è´¥é¡µé¢: %d",
            pages_processed,
            len(all_records),
            failed_pages,
        )
        
        return all_records


    async def download_image(self, image_url: str) -> Optional[str]:
        """ä¸‹è½½å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•"""
        try:
            if not image_url:
                logger.error("å›¾ç‰‡URLä¸ºç©º")
                return None

            # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„
            file_extension = os.path.splitext(urlparse(image_url).path)[1] or ".jpg"
            temp_file_path = os.path.join(
                self.temp_dir,
                f"91vip_image_{random.randint(1000, 9999)}{file_extension}",
            )

            # ç¡®ä¿HTTPå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            if not self.http_client:
                await self.initialize_async()

            # ä¸‹è½½å›¾ç‰‡
            async with self.http_client.get(image_url) as response:
                if response.status != 200:
                    logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    return None

                content = await response.read()
                with open(temp_file_path, "wb") as f:
                    f.write(content)

            logger.info(f"å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {temp_file_path}")
            return temp_file_path

        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            return None

    async def censor_image(self, image_path: str) -> str:
        """å¯¹å›¾ç‰‡è¿›è¡Œæ‰“ç å¤„ç†"""
        try:
            if not image_path or not os.path.exists(image_path):
                logger.error("å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨")
                return ""

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ‰“ç åŠŸèƒ½
            if not self.config.get("image_censorship_enabled", True):
                logger.info("å›¾ç‰‡æ‰“ç åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›åŸå›¾")
                return image_path

            # æ‰“å¼€å›¾ç‰‡
            with Image.open(image_path) as img:
                # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå¦‚æœæ˜¯RGBAæˆ–å…¶ä»–æ¨¡å¼ï¼‰
                if img.mode != "RGB":
                    img = img.convert("RGB")

                # è·å–å›¾ç‰‡å°ºå¯¸
                width, height = img.size

                # è®¡ç®—é©¬èµ›å…‹å—å¤§å°ï¼ˆåŸºäºå›¾ç‰‡å°ºå¯¸çš„ç™¾åˆ†æ¯”ï¼‰
                mosaic_level = self.config.get("mosaic_level", 0.8)
                if mosaic_level <= 0 or mosaic_level > 1:
                    mosaic_level = 0.8

                # æ ¹æ®é©¬èµ›å…‹ç¨‹åº¦è®¡ç®—å—å¤§å°
                # é©¬èµ›å…‹ç¨‹åº¦è¶Šé«˜ï¼Œå—å¤§å°è¶Šå¤§
                block_size = int(
                    min(width, height) * mosaic_level * 0.05
                )  # 5% * é©¬èµ›å…‹ç¨‹åº¦
                block_size = max(block_size, 5)  # æœ€å°å—å¤§å°ä¸º5åƒç´ 

                # åˆ›å»ºé©¬èµ›å…‹æ•ˆæœ
                for y in range(0, height, block_size):
                    for x in range(0, width, block_size):
                        # è·å–å½“å‰å—çš„å¹³å‡é¢œè‰²
                        block = img.crop((x, y, x + block_size, y + block_size))
                        if block.size[0] > 0 and block.size[1] > 0:
                            # è®¡ç®—å¹³å‡é¢œè‰²
                            avg_color = tuple(
                                int(sum(c) / len(c)) for c in zip(*block.getdata())
                            )

                            # åˆ›å»ºçº¯è‰²å—
                            solid_block = Image.new(
                                "RGB", (block_size, block_size), avg_color
                            )
                            img.paste(solid_block, (x, y))

                # ä¿å­˜æ‰“ç åçš„å›¾ç‰‡
                censored_path = os.path.join(
                    self.temp_dir, f"censored_{os.path.basename(image_path)}"
                )
                img.save(censored_path, "JPEG", quality=85)

            # åˆ é™¤åŸå§‹å›¾ç‰‡
            try:
                os.remove(image_path)
            except Exception as e:
                logger.warning(f"åˆ é™¤åŸå§‹å›¾ç‰‡å¤±è´¥: {e}")

            logger.info(f"å›¾ç‰‡æ‰“ç å®Œæˆ: {censored_path}")
            return censored_path

        except Exception as e:
            logger.error(f"å›¾ç‰‡æ‰“ç å¤±è´¥: {e}")
            # å¦‚æœæ‰“ç å¤±è´¥ï¼Œå°è¯•åˆ é™¤åŸå§‹å›¾ç‰‡
            try:
                if os.path.exists(image_path):
                    os.remove(image_path)
            except Exception as e:
                logger.warning(f"åˆ é™¤åŸå§‹å›¾ç‰‡å¤±è´¥: {e}")
            return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ‰“ç å¤±è´¥

    async def get_91porn_thumbnails(self, category: str = "rf", max_thumbnails: int = 5) -> List[Dict[str, str]]:
        """è·å–91pornè§†é¢‘ç¼©ç•¥å›¾"""
        try:
            # ç¡®ä¿HTTPå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            if not self.http_client:
                await self.initialize_async()

            # æ„å»ºURL
            url = build_page_url(category, "basic", 1)
            
            # è·å–é¡µé¢å†…å®¹
            async with self.http_client.get(url) as response:
                if response.status != 200:
                    logger.error(f"è·å–é¡µé¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # æŸ¥æ‰¾è§†é¢‘å¡ç‰‡
                cards = soup.select(VIDEO_CARD_SELECTOR)
                thumbnails = []
                
                for card in cards[:max_thumbnails]:
                    try:
                        # è·å–æ ‡é¢˜
                        title_el = card.select_one(TITLE_SELECTOR)
                        title = title_el.text.strip() if title_el and title_el.text else "æ— æ ‡é¢˜"
                        
                        # è·å–é“¾æ¥
                        link_el = card.find("a")
                        link = getattr(link_el, "get", lambda *_: None)("href")
                        
                        # è·å–å›¾ç‰‡
                        img_el = card.find("img")
                        img_src = getattr(img_el, "get", lambda *_: None)("src")
                        
                        if link and img_src:
                            # ç¡®ä¿å›¾ç‰‡URLæ˜¯å®Œæ•´çš„
                            if img_src.startswith("//"):
                                img_src = "https:" + img_src
                            elif img_src.startswith("/"):
                                img_src = "https://91porn.com" + img_src
                            
                            thumbnails.append({
                                "title": title,
                                "link": link,
                                "image_url": img_src
                            })
                    except Exception as e:
                        logger.warning(f"è§£æè§†é¢‘å¡ç‰‡å¤±è´¥: {e}")
                        continue
                
                return thumbnails
                
        except Exception as e:
            logger.error(f"è·å–91pornç¼©ç•¥å›¾å¤±è´¥: {e}")
            return []

    # æ³¨å†ŒæŒ‡ä»¤ï¼šçˆ¬å–91pornè§†é¢‘åˆ—è¡¨ï¼ˆç›´æ¥è¿”å›å†…å®¹ï¼‰
    @filter.command("91porn", alias={'91', 'çˆ¬å–è§†é¢‘'})
    async def scrape_91porn(self, event: AstrMessageEvent, category: str = None, count: int = None):
        """çˆ¬å–91pornè§†é¢‘åˆ—è¡¨å¹¶ç›´æ¥è¿”å›ç»“æœ
        
        ç”¨æ³•: /91porn [åˆ†ç±»] [æ•°é‡]
        åˆ†ç±»: rf(çƒ­é—¨), mv(æœ€æ–°), vd(è§†é¢‘)ç­‰ï¼Œé»˜è®¤ä¸ºrf
        æ•°é‡: è¦è·å–çš„è§†é¢‘æ•°é‡ï¼Œé»˜è®¤ä¸º5ï¼ˆå»ºè®®ä¸è¦è¶…è¿‡10ä¸ªï¼‰
        
        ç¤ºä¾‹:
        /91porn rf 5
        /91porn mv 3
        /91porn
        """
        user_name = event.get_sender_name()
        
        try:
            # æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
            if not self.config.get("91porn_enabled", True):
                yield event.plain_result(f"âŒ {user_name}, 91pornçˆ¬è™«åŠŸèƒ½å·²ç¦ç”¨")
                return
            
            # ä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼Œä½†é™åˆ¶æ•°é‡ä»¥é¿å…æ¶ˆæ¯è¿‡é•¿
            category = category or self.config.get("91porn_category", "rf")
            count = min(count or 5, 10)  # é™åˆ¶æœ€å¤§10ä¸ª
            
            yield event.plain_result(f"ğŸ” {user_name}, æ­£åœ¨è·å–{category}åˆ†ç±»çš„{count}ä¸ªè§†é¢‘ï¼Œè¯·ç¨å€™...")
            
            # ç›´æ¥æ‰§è¡Œçˆ¬å–ï¼Œä¸ä½¿ç”¨ä»»åŠ¡ç³»ç»Ÿ
            try:
                # è®°å½•çˆ¬å–å¼€å§‹
                start_time = time.time()
                logger.info(f"å¼€å§‹ç›´æ¥çˆ¬å–: {category} åˆ†ç±»ï¼Œè·å– {count} ä¸ªè§†é¢‘")
                
                # è·å–ç¼©ç•¥å›¾åˆ—è¡¨
                thumbnails = await self.get_91porn_thumbnails(category, count)
                
                if thumbnails:
                    # å¤„ç†æ¯ä¸ªç¼©ç•¥å›¾
                    processed_thumbnails = []
                    for i, thumbnail in enumerate(thumbnails, 1):
                        try:
                            # ä¸‹è½½å›¾ç‰‡
                            image_path = await self.download_image(thumbnail["image_url"])
                            if not image_path:
                                logger.warning(f"ç¬¬{i}ä¸ªå°é¢ä¸‹è½½å¤±è´¥")
                                continue
                            
                            # æ‰“ç å¤„ç†
                            censored_image_path = await self.censor_image(image_path)
                            if not censored_image_path:
                                logger.warning(f"ç¬¬{i}ä¸ªå°é¢å¤„ç†å¤±è´¥")
                                continue
                            
                            processed_thumbnails.append({
                                "title": thumbnail["title"],
                                "link": thumbnail["link"],
                                "image_path": censored_image_path
                            })
                            
                        except Exception as e:
                            logger.error(f"å¤„ç†ç¬¬{i}ä¸ªå°é¢å¤±è´¥: {e}")
                            continue
                    
                    crawl_duration = time.time() - start_time
                    
                    if processed_thumbnails:
                        # æ ¼å¼åŒ–ç»“æœ
                        message = f"âœ… {user_name}, è·å–å®Œæˆï¼\n\n"
                        message += f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\n"
                        message += f"   ğŸ·ï¸ åˆ†ç±»: {category}\n"
                        message += f"   ğŸ“ è§†é¢‘æ•°: {len(processed_thumbnails)}\n"
                        message += f"   â±ï¸ è€—æ—¶: {crawl_duration:.1f}ç§’\n\n"
                        
                        message += f"ğŸ“‹ è§†é¢‘åˆ—è¡¨:\n\n"
                        
                        # å‘é€æ¯ä¸ªè§†é¢‘çš„ä¿¡æ¯å’Œå›¾ç‰‡
                        for i, thumbnail in enumerate(processed_thumbnails, 1):
                            # å‘é€å›¾ç‰‡
                            yield event.image_result(thumbnail["image_path"])
                            
                            # å‘é€è§†é¢‘ä¿¡æ¯
                            info_text = f"ğŸ“¹ è§†é¢‘ {i}/{len(processed_thumbnails)}\n"
                            info_text += f"ğŸ·ï¸ æ ‡é¢˜: {thumbnail['title']}\n"
                            info_text += f"ğŸ”— é“¾æ¥: {thumbnail['link']}"
                            yield event.plain_result(info_text)
                    else:
                        yield event.plain_result(f"âŒ {user_name}, æ‰€æœ‰è§†é¢‘å°é¢å¤„ç†å¤±è´¥")
                else:
                    yield event.plain_result(f"âŒ {user_name}, æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘")
                
                logger.info(f"ç›´æ¥çˆ¬å–å®Œæˆ: {len(processed_thumbnails) if 'processed_thumbnails' in locals() else 0} ä¸ªè§†é¢‘ï¼Œè€—æ—¶ {crawl_duration:.1f} ç§’")
                
            except Exception as e:
                logger.error(f"ç›´æ¥çˆ¬å–å¤±è´¥: {e}")
                yield event.plain_result(f"âŒ {user_name}, çˆ¬å–å¤±è´¥: {str(e)}")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨91pornçˆ¬å–å¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å¯åŠ¨çˆ¬å–å¤±è´¥: {str(e)}")




    async def terminate(self):
        """å¯é€‰æ‹©å®ç°å¼‚æ­¥çš„æ’ä»¶é”€æ¯æ–¹æ³•ï¼Œå½“æ’ä»¶è¢«å¸è½½/åœç”¨æ—¶ä¼šè°ƒç”¨ã€‚"""
        logger.info("91pornçˆ¬è™«æ’ä»¶æ­£åœ¨å…³é—­...")
        
        # å…³é—­HTTPå®¢æˆ·ç«¯
        if self.http_client:
            await self.http_client.close()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info("ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        
        self.executor.shutdown(wait=True)
        logger.info("91pornçˆ¬è™«æ’ä»¶å·²å…³é—­")